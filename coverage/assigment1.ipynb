{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "minst = fetch_openml('mnist_784', cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = minst.data.astype('float32')\n",
    "y = minst.target.astype('int64')\n",
    "x['target'] = y\n",
    "X = []\n",
    "for i in range(10):\n",
    "  X.append(x[x['target']==i])\n",
    "  X[i] = X[i].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "# train ratio is 70%\n",
    "# test ratio is 15%\n",
    "# validation ratio is 15%\n",
    "# transform the data into a torch tensor and normalize it to be between 0 and 1\n",
    "rowRange = y.value_counts()\n",
    "train = []\n",
    "test = []\n",
    "validation = []\n",
    "trainY = []\n",
    "validationY = []\n",
    "testY = []\n",
    "\n",
    "def transform(array):\n",
    "    array[i] = array[i].to_numpy()\n",
    "    array[i] = torch.Tensor(array[i])\n",
    "    array[i] = array[i]/255\n",
    "\n",
    "def getSizes(index):\n",
    "    trainSize = int(0.7 * len(X[index]))\n",
    "    validationSize = int(0.15 * len(X[index]))\n",
    "    testSize = len(X[index]) - trainSize - validationSize\n",
    "    return trainSize, validationSize, testSize\n",
    "\n",
    "for i in range(10):\n",
    "    trainSize, validationSize, testSize = getSizes(i)\n",
    "    train.append(X[i][:trainSize])\n",
    "    validation.append(X[i][trainSize:trainSize+validationSize])\n",
    "    test.append(X[i][trainSize+validationSize:])\n",
    "\n",
    "    trainY.append(train[i]['target'].to_numpy())\n",
    "    validationY.append(validation[i]['target'].to_numpy())\n",
    "    testY.append(test[i]['target'].to_numpy())\n",
    "    # cleanup\n",
    "    del train[i]['target']\n",
    "    del validation[i]['target']\n",
    "    del test[i]['target']\n",
    "    transform(train)\n",
    "    transform(test)\n",
    "    transform(validation)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification via KNN for 2 and 3 combination\n",
    "# 2 is the positive class and 3 is the negative class\n",
    "# implement the KNN algorithm\n",
    "# use the Euclidean distance and the Manhattan distance\n",
    "# use the KNN algorithm to classify the test data\n",
    "\n",
    "# create data and label concatenation 2 and 3\n",
    "\n",
    "data = torch.cat((train[2], train[3]), 0)\n",
    "dataLabel = np.concatenate((trainY[2], trainY[3]), 0)\n",
    "validation = torch.cat((validation[2], validation[3]), 0)\n",
    "validationLabel = np.concatenate((validationY[2], validationY[3]), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 2 ANSWER (a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.34"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm(x,y, type):\n",
    "  if(type==\"L1\"):\n",
    "    return torch.norm(x-y, p=1, dim=1)\n",
    "  elif(type==\"L2\"):\n",
    "    return torch.norm(x-y, p=2, dim=1)\n",
    "\n",
    "def knn(data, dataLabel, validation, validationLabel, k, distance_type):\n",
    "  hitCount = 0\n",
    "  for target in range(len(validation)):\n",
    "    result = norm(data, validation[target], distance_type)\n",
    "    # get the k smallest distances\n",
    "    knn = result.topk(k, largest=False)\n",
    "    count = Counter(dataLabel[knn.indices])\n",
    "    hit = count.most_common(1)[0][0]\n",
    "    if(hit == validationLabel[target]):\n",
    "      hitCount += 1\n",
    "  return round(hitCount/len(validation)*100, 2)\n",
    "\n",
    "print(\"Problem 2 ANSWER (a)\")\n",
    "pred = knn(data, dataLabel, validation, validationLabel, 5, \"L1\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 b) What are the hyperparameters you can tune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) What are the hyperparameters you can tune?\n",
    "# k, distance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Try at least two other options for each hyperparameter. Report the performance for\n",
    "each option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem2 - (c)\n",
      "K  3\n",
      "L1  99.48\n",
      "L2  99.67\n",
      "K  5\n",
      "L1  99.34\n",
      "L2  99.43\n"
     ]
    }
   ],
   "source": [
    "# c) Try at least two other options for each hyperparameter. Report the performance for\n",
    "# each option.\n",
    "print(\"Problem 2 ANSWER (c)\")\n",
    "for k in range(3,7,2):\n",
    "    predL1 = knn(data, dataLabel, validation, validationLabel, k, \"L1\")\n",
    "    predL2 = knn(data, dataLabel, validation, validationLabel, k, \"L2\")\n",
    "    print(\"K \", k)\n",
    "    print(\"L1 (Manhatten Distance): \", predL1)\n",
    "    print(\"L2 (Euclidean Distance): \", predL2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) You can try more options if you want. What is the final test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  3\n",
      "L2 (Euclidean Distance):  99.48\n"
     ]
    }
   ],
   "source": [
    "# d) You can try more options if you want. What is the final test accuracy?\n",
    "predL2 = knn(data, dataLabel, validation, validationLabel, 3, \"L2\")\n",
    "print(\"K \", 3)\n",
    "print(\"L2 (Euclidean Distance): \", predL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem #3: Multiclass Classification via k-NN [15pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Make a k-NN (k=5) and its training/validation/evaluation code to perform multiclass\n",
    "classification over all digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Make a k-NN (k=5) and its training/validation/evaluation code to perform multiclass\n",
    "# classification over all digits.\n",
    "data = torch.cat((train[0], train[1]), 0)\n",
    "dataLabel = np.concatenate((trainY[0], trainY[1]), 0)\n",
    "validation = torch.cat((validation[0], validation[1]), 0)\n",
    "validationLabel = np.concatenate((validationY[0], validationY[1]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 ANSWER (a)\n",
      "Multiclass KNN Accuracy:  99.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Problem 3 ANSWER (a)\")\n",
    "Pred = knn(data, dataLabel, validation, validationLabel, 5, \"L2\")\n",
    "print(\"Multiclass KNN Accuracy: \", Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the hyperparameters you can tune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) What are the hyperparameters you can tune?\n",
    "# k, distance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Try at least two other options for each hyperparameter. Report the performance for\n",
    "each option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 ANSWER (c)\n",
      "K  3\n",
      "L1 (Manhatten Distance):  99.06\n",
      "L2 (Euclidean Distance):  99.15\n",
      "K  5\n",
      "L1 (Manhatten Distance):  99.15\n",
      "L2 (Euclidean Distance):  99.2\n"
     ]
    }
   ],
   "source": [
    "# c) Try at least two other options for each hyperparameter. Report the performance for\n",
    "# each option.\n",
    "print(\"Problem 3 ANSWER (c)\")\n",
    "for k in range(3,7,2):\n",
    "    predL1 = knn(data, dataLabel, validation, validationLabel, k, \"L1\")\n",
    "    predL2 = knn(data, dataLabel, validation, validationLabel, k, \"L2\")\n",
    "    print(\"K \", k)\n",
    "    print(\"L1 (Manhatten Distance): \", predL1)\n",
    "    print(\"L2 (Euclidean Distance): \", predL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 ANSWER (d)\n",
      "K  3\n",
      "Multiclass L2 (Euclidean Distance) (AUC):  99.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Problem 3 ANSWER (d)\")\n",
    "# d) You can try more options if you want. What is the final test accuracy?\n",
    "predL2 = knn(data, dataLabel, validation, validationLabel, 3, \"L2\")\n",
    "print(\"K \", 3)\n",
    "print(\"Multiclass L2 (Euclidean Distance) (AUC): \", predL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        return torch.mean(torch.clamp(1 - y * y_pred, min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(X, Y, model):\n",
    "def SVMAccuracy(X, Y, model):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        y_pred[y_pred >= 0] = 1\n",
    "        y_pred[y_pred < 0] = -1\n",
    "        accuracy = (y_pred == Y).sum().item() / len(Y)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def svm(X, Y, val, val_label, learning_rate, epoch, batch_size)\n",
    "def svm(X, Y, val, val_label, learning_rate, epoch, batch_size):\n",
    "    model = nn.Linear(784, 1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "    criterion = HingeLoss()\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X[j:j+batch_size])\n",
    "            loss = criterion(Y[j:j+batch_size], y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if(i%10 == 0):\n",
    "            print(\"Epoch: \", i, \" Loss: \", loss.item(), \" Accuracy: \", SVMAccuracy(val, val_label, model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((train[2], train[3]), 0)\n",
    "Y = np.concatenate((trainY[2], trainY[3]), 0)\n",
    "Y[Y==2] = -1\n",
    "Y[Y==3] = 1\n",
    "validation = torch.cat((validation[2], validation[3]), 0)\n",
    "validationLabel = np.concatenate((validationY[2], validationY[3]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem4 - (b)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_143382/886752501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem4 - (b)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_143382/19248716.py\u001b[0m in \u001b[0;36msvm\u001b[0;34m(X, Y, val, val_label, learning_rate, epoch, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143382/2719522258.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y, y_pred)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "print(\"Problem4 - (b)\")\n",
    "# HAVE Problem at my computer to run\n",
    "svm(X, Y, validation, validationLabel, 0.1, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem #4: Binary Classification via soft-margin SVM [15pts]\n",
    "# had a problem running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
